const html = String.raw;
/**
 * Returns sample text for the history of computing.
 * @returns {string} Sample text.
 */
export const HistoryOfComputing = html`<p>
        The history of computing is a journey of human ingenuity that spans centuries, transforming the way we
        live, work, and think. From ancient tools to modern quantum computers, each milestone reflects a leap
        in our understanding of logic, mathematics, and machinery. Here are some of the most amazing facts
        from this incredible evolution: 10 Amazing Facts from the History of Computing:
    </p>
    <ol>
        <li>
            The first "computer programmer" lived in the 1800s. Ada Lovelace wrote an algorithm for
            <a href="https://en.wikipedia.org/wiki/Charles_Babbage" target="_blank">Charles Babbage's</a>
            Analytical Engine in 1843—long before modern computers existed.
        </li>
        <li>
            The first mechanical computer was never built. Charles Babbage designed the Analytical Engine in
            the 1830s, but due to funding and technological limits, it remained on paper.
        </li>
        <li>
            ENIAC weighed 27 tons. Completed in 1945, it was one of the first general-purpose computers and
            used over 17,000 vacuum tubes.
        </li>
        <li>
            The first computer bug was a real moth. In 1947, engineers found a moth causing issues in the
            Harvard Mark II computer, coining the term “debugging.”
        </li>
        <li>
            Computers helped crack Nazi codes. Alan Turing and his team at Bletchley Park developed early
            computing machines to break the Enigma cipher, shortening WWII.
        </li>
        <li>
            The mouse was invented in the 1960s. Douglas Engelbart created the first computer mouse out of
            wood in 1964 as part of his vision for interactive computing.
        </li>
        <li>
            Moore's Law predicted exponential growth. Gordon Moore predicted in 1965 that the number of
            transistors on a chip would double every two years—a trend that held for decades.
        </li>
        <li>
            Apple started in a garage. Steve Jobs and Steve Wozniak built the first Apple computer in a garage
            in 1976, kicking off a personal computing revolution.
        </li>
        <li>
            Linux was started by a student. In 1991, Linus Torvalds, a Finnish student, created Linux as a
            hobby project—now it powers everything from servers to Android phones.
        </li>
        <li>
            Quantum computing is now a reality. Once a theoretical concept, quantum computers now exist and
            are being developed by companies like IBM and Google to solve problems classical computers can't
            handle.
        </li>
    </ol>`;

export const AmazingComputingFacts = html`<p>
        From the strange world of quantum mechanics to the limits of physical computation, the science behind
        computing is full of astonishing discoveries. These facts show how computing isn't just about code and
        machines—it's deeply connected to physics, mathematics, and even biology.
    </p>
    <ol>
        <li>
            <strong>Information has mass.</strong> According to physics, erasing a bit of information produces
            heat—this is known as
            <a href="https://en.wikipedia.org/wiki/Landauer%27s_principle" target="_blank"
                >Landauer's Principle</a
            >. That means information is physical!
        </li>
        <li>
            <strong>Quantum computers can be in multiple states at once.</strong> Thanks to quantum
            superposition, a quantum bit (qubit) can represent 0 and 1 simultaneously, allowing quantum
            computers to process vast possibilities in parallel.
        </li>
        <li>
            <strong>DNA can store data.</strong> Scientists have encoded and retrieved digital data—images,
            videos, even a computer OS—into strands of DNA, offering storage densities millions of times
            greater than current hardware.
        </li>
        <li>
            <strong>The universe may be a giant computer.</strong> Theoretical physicist Konrad Zuse proposed
            in 1969 that the universe is being computed on a fundamental level—a concept now explored in
            digital physics.
        </li>
        <li>
            <strong>There's a mathematical limit to what computers can solve.</strong> Some problems are
            undecidable, like the famous
            <a href="https://en.wikipedia.org/wiki/Halting_problem" target="_blank">Halting Problem</a>,
            meaning no algorithm can ever solve them for all inputs.
        </li>
        <li>
            <strong>The fastest computers simulate the human brain—barely.</strong> As of now, even the most
            powerful supercomputers take minutes or hours to simulate just one second of real-time brain
            activity.
        </li>
        <li>
            <strong>Light can carry out computations.</strong> Optical computing uses photons instead of
            electrons, promising faster speeds and less heat—still an experimental but exciting frontier.
        </li>
        <li>
            <strong>Computing power once doubled every 18 months.</strong> This exponential growth, known as
            Moore's Law, led to the rapid evolution of technology—but is now slowing due to physical limits.
        </li>
        <li>
            <strong>AI can now write its own code.</strong> Advances in machine learning have enabled AI
            systems to generate and improve code—blurring the line between programmer and program.
        </li>
        <li>
            <strong>Some algorithms are faster than hardware improvements.</strong> In some cases, a better
            algorithm can speed up computation more than a hardware upgrade—a reminder of the power of human
            thought in computing.
        </li>
    </ol> `;

export const ApolloMission = html`<p>
        The Apollo missions didn’t just take humanity to the Moon—they also showcased some of the most
        impressive computing feats of the time. In an era before smartphones or personal computers, NASA
        engineers and programmers broke new ground with the limited technology they had.
    </p>
    <ol>
        <li>
            <strong>The Apollo Guidance Computer (AGC) had less memory than a calculator.</strong> It had just
            64KB of memory and operated at 0.043 MHz—yet it got humans safely to the Moon and back.
        </li>
        <li>
            <strong>The AGC was the first computer to use integrated circuits (ICs).</strong> This
            cutting-edge choice helped miniaturize the system, making spaceflight computing even possible.
        </li>
        <li>
            <strong>Programs were literally woven into memory.</strong> The AGC used
            <a href="https://en.wikipedia.org/wiki/Core_rope_memory" target="_blank">core rope memory</a>,
            where wires were threaded through magnetic rings by hand—mostly by women in textile factories.
        </li>
        <li>
            <strong>It used cooperative multitasking.</strong> The AGC could pause low-priority tasks to
            handle more urgent ones—an early form of real-time computing that helped avoid crashes during
            landing.
        </li>
        <li>
            <strong>A software reboot saved Apollo 11’s landing.</strong> During descent, the AGC was
            overloaded with radar data, but its design allowed it to reboot and continue critical
            tasks—avoiding mission failure.
        </li>
        <li>
            <strong>Much of the software was written by hand.</strong> Programmers, including
            <a href="https://en.wikipedia.org/wiki/Margaret_Hamilton_(software_engineer)" target="_blank"
                >Margaret Hamilton</a
            >, manually wrote thousands of lines of assembly code to control everything from navigation to
            landing.
        </li>
        <li>
            <strong>The AGC had a custom user interface called the DSKY.</strong> Short for “Display and
            Keyboard,” it allowed astronauts to enter numerical commands using verbs and nouns, like “Verb 06
            Noun 62.”
        </li>
        <li>
            <strong>The code had to be bug-free—literally.</strong> With no ability to patch or update once
            launched, NASA had to ensure near-perfect reliability in both hardware and software before
            liftoff.
        </li>
        <li>
            <strong>The AGC was a pioneer in human-computer interaction.</strong> It’s one of the earliest
            examples of a digital system designed specifically to interact with humans in life-or-death
            conditions.
        </li>
        <li>
            <strong>Modern phones are millions of times more powerful.</strong> Despite their limited power,
            the Apollo computers successfully supported navigation, communication, and control—an incredible
            feat of efficiency and precision.
        </li>
    </ol>`;

export const VideoGameHistory = html`<p>
        From humble beginnings with pixelated screens to today's photorealistic, immersive worlds, video games
        have grown into a global phenomenon. Their history is packed with innovation, creativity, and
        unexpected breakthroughs. Here are some of the most amazing facts about the history of video games.
    </p>
    <ol>
        <li>
            <strong>The first video game was made in the 1950s.</strong> In 1958, physicist William
            Higinbotham created <em>Tennis for Two</em> on an oscilloscope—predating Pong by over a decade.
        </li>
        <li>
            <strong>Pong started the video game industry.</strong> Released in 1972 by Atari,
            <em>Pong</em> was one of the first commercially successful games and helped launch the arcade era.
        </li>
        <li>
            <strong>Mario was originally called “Jumpman.”</strong> In <em>Donkey Kong</em> (1981), Mario was
            a nameless carpenter called Jumpman—he became a plumber and got his name later.
        </li>
        <li>
            <strong>Pac-Man was inspired by pizza.</strong> Creator Toru Iwatani got the idea for Pac-Man’s
            shape after taking a slice from a pizza—and wanted to make a game that appealed to women.
        </li>
        <li>
            <strong>There’s a video game buried in the desert.</strong> In 1983, unsold copies of Atari’s
            <em>E.T.</em> were dumped in a New Mexico landfill—a legendary moment that symbolized the video
            game crash of the early '80s.
        </li>
        <li>
            <strong>The Konami Code became a cultural icon.</strong> The cheat code ↑ ↑ ↓ ↓ ← → ← → B A became
            famous in games like <em>Contra</em> and has appeared in dozens of games since.
        </li>
        <li>
            <strong>Handheld gaming began in the 1980s.</strong> Nintendo’s <em>Game & Watch</em> series,
            created by Gunpei Yokoi, laid the foundation for the Game Boy and modern portable consoles.
        </li>
        <li>
            <strong><em>Doom</em> popularized first-person shooters and modding.</strong> Released in 1993,
            <em>Doom</em> was a landmark game that allowed players to create and share custom levels.
        </li>
        <li>
            <strong>Video games are now bigger than Hollywood.</strong> The global gaming industry generates
            more revenue than the movie and music industries combined—over $180 billion annually.
        </li>
        <li>
            <strong>The oldest video game easter egg is from 1980.</strong> In <em>Adventure</em> for the
            Atari 2600, a hidden room credits the game’s creator—because developers weren’t acknowledged back
            then.
        </li>
    </ol> `;

export const SoftwareEngineer = html`<p>
        Being a great software engineer isn't just about writing code—it's about mindset, discipline, and
        continuous growth. Whether you're just starting or deep into your career, these principles can help
        guide you through challenges, keep you sharp, and make your work more impactful and sustainable.
    </p>
    <ol>
        <li>
            <strong>Write code for humans, not just machines.</strong> Clear, readable code is more valuable
            than clever code. Always prioritize maintainability and communication over complexity. (<a
                href="https://www.youtube.com/watch?v=HZH0x6HjFxA"
                target="_blank"
                >Clean Code by Uncle Bob</a
            >)
        </li>
        <li>
            <strong>Learn continuously.</strong> Technology evolves fast. Stay curious, humble, and open to
            new tools, ideas, and paradigms—no matter how experienced you are. (<a
                href="https://roadmap.sh"
                target="_blank"
                >Roadmap.sh: Developer Learning Paths</a
            >)
        </li>
        <li>
            <strong>Understand the problem before solving it.</strong> Jumping into code too soon can lead to
            wasted effort. Spend time analyzing and designing the right solution. (<a
                href="https://martinfowler.com/articles/requirements.html"
                target="_blank"
                >Martin Fowler: Requirements and Analysis</a
            >)
        </li>
        <li>
            <strong>Don’t be afraid to ask questions.</strong> Great engineers seek clarity. Asking questions
            early prevents mistakes later and shows a commitment to quality. (<a
                href="https://stackoverflow.blog/2019/10/01/want-to-be-a-better-developer-learn-to-ask-better-questions/"
                target="_blank"
                >Stack Overflow: Learn to Ask Better Questions</a
            >)
        </li>
        <li>
            <strong>Think in systems, not just features.</strong> Every decision affects performance,
            scalability, security, and maintainability. Think beyond the immediate task. (<a
                href="https://12factor.net"
                target="_blank"
                >The Twelve-Factor App</a
            >)
        </li>
        <li>
            <strong>Test your code like you don’t trust it.</strong> Automated testing, thoughtful edge cases,
            and reproducible bugs are part of writing professional-grade software. (<a
                href="https://kentcdodds.com/blog/write-tests"
                target="_blank"
                >Kent C. Dodds: Write Tests</a
            >)
        </li>
        <li>
            <strong>Embrace feedback—both giving and receiving.</strong> Code reviews are a chance to learn
            and improve, not a personal critique. Treat them as collaborative tools for growth. (<a
                href="https://google.github.io/eng-practices/review/"
                target="_blank"
                >Google’s Engineering Code Review Guide</a
            >)
        </li>
        <li>
            <strong>Be mindful of tech debt.</strong> It’s okay to ship quickly, but always plan for cleanup.
            Tech debt compounds if ignored. (<a
                href="https://martinfowler.com/bliki/TechnicalDebt.html"
                target="_blank"
                >Martin Fowler: Technical Debt</a
            >)
        </li>
        <li>
            <strong>Know the business value of what you're building.</strong> Understanding the “why” behind
            your work helps you make smarter technical decisions and become a more strategic engineer. (<a
                href="https://basecamp.com/shapeup"
                target="_blank"
                >Basecamp's Shape Up: Prioritizing Business Value</a
            >)
        </li>
        <li>
            <strong>Take care of your health and balance.</strong> Burnout helps no one. Sustainable
            productivity, boundaries, and time away from the screen are just as important as shipping code.
            (<a href="https://www.notion.so/blog/avoiding-burnout" target="_blank">Avoiding Burnout in Tech</a
            >)
        </li>
    </ol> `;
